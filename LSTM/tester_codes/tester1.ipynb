{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # We need numpy to handle matrices of numbers and data\n",
    "import tensorflow as tf # Tensorflow is the library that we will be using for simulating an LSTM model\n",
    "import pandas as pd # Pandas library are for reading .csv files or other excel files\n",
    "from matplotlib import pyplot as plt # Matplotlib for visualizing data or plotting it\n",
    "from sklearn.preprocessing import MinMaxScaler # This is for scaling the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reads the csv file and saves it in a variable as a pandas data frame\n",
    "df = pd.read_csv('../datas/train_passengers_copy.csv')\n",
    "\n",
    "#Variables for training\n",
    "cols = list(df)[1:3]\n",
    "df_for_training = df[cols].astype(float)\n",
    "\n",
    "#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_for_training)\n",
    "df_for_training_scaled = scaler.transform(df_for_training)\n",
    "\n",
    "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "#In this example, the n_features is 5. We will make timesteps = 14 (past days data used for training). \n",
    "\n",
    "#Empty lists to be populated using formatted training data\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "n_future = 1   # Number of days we want to look into the future based on the past days.\n",
    "n_past = 3  # Number of past days we want to use to predict the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape == (141, 3, 2).\n",
      "trainY shape == (141, 1).\n"
     ]
    }
   ],
   "source": [
    "#Reformat input data into a shape: (n_samples x timesteps x n_features)\n",
    "#In my example, my df_for_training_scaled has a shape (12823, 5)\n",
    "#12823 refers to the number of data points and 5 refers to the columns (multi-variables).\n",
    "for i in range(n_past, len(df_for_training_scaled) - n_future +1):\n",
    "    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])\n",
    "    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "print('trainX shape == {}.'.format(trainX.shape))\n",
    "print('trainY shape == {}.'.format(trainY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = tf.keras.models.load_model(\"../saved_param/model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
       " array([[-0.8367793],\n",
       "        [-0.4085069],\n",
       "        [ 1.1929541]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.01844767], dtype=float32)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.layers[1].trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.06236403]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.predict(np.array([trainX[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01544402, 0.8       ],\n",
       "       [0.02702703, 0.8       ],\n",
       "       [0.05405405, 0.6       ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 3\n",
    "W = load_model.layers[0].get_weights()[0]\n",
    "U = load_model.layers[0].get_weights()[1]\n",
    "b = load_model.layers[0].get_weights()[2]\n",
    "\n",
    "W_i = W[:, :units]\n",
    "W_f = W[:, units: units * 2]\n",
    "W_c = W[:, units * 2: units * 3]\n",
    "W_o = W[:, units * 3:]\n",
    "\n",
    "U_i = U[:, :units]\n",
    "U_f = U[:, units: units * 2]\n",
    "U_c = U[:, units * 2: units * 3]\n",
    "U_o = U[:, units * 3:]\n",
    "\n",
    "b_i = b[:units]\n",
    "b_f = b[units: units * 2]\n",
    "b_c = b[units * 2: units * 3]\n",
    "b_o = b[units * 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10793971  0.81825334  0.6556316 ]\n",
      " [-0.60634685 -0.6131416  -0.49448305]]\n",
      "[[ 0.5622357   0.93071234  0.46856326]\n",
      " [ 0.37466365 -0.48716673 -0.1355884 ]\n",
      " [-0.52040434  0.1333436   0.87895346]]\n",
      "[-0.22539224  0.01720281  0.21763435]\n"
     ]
    }
   ],
   "source": [
    "print(W_i)\n",
    "print(U_i)\n",
    "print(b_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.56221163 -0.01619006 -0.14644638]\n",
      " [-0.3923156   0.25872135  0.4800443 ]]\n",
      "[[-0.02404333 -0.03320475  0.2481951 ]\n",
      " [-0.16247411 -0.07040676  0.17257127]\n",
      " [-0.07867586  0.20210117 -0.24236679]]\n",
      "[0.7972173 1.0329572 1.1679661]\n"
     ]
    }
   ],
   "source": [
    "print(W_f)\n",
    "print(U_f)\n",
    "print(b_f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34074333 -0.36218873  0.87516445]\n",
      " [-0.16738938 -0.27611265 -0.06332847]]\n",
      "[[ 0.12381045 -0.23511174  0.4601762 ]\n",
      " [ 0.43099013  0.2494406  -0.18612424]\n",
      " [ 0.00344216 -0.25496334  0.04300574]]\n",
      "[-0.02486727 -0.06337232 -0.04763215]\n"
     ]
    }
   ],
   "source": [
    "print(W_c)\n",
    "print(U_c)\n",
    "print(b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3573572   0.20927963  0.44177994]\n",
      " [-0.27215135  0.3107963   0.27268055]]\n",
      "[[ 0.12031795  0.30339935  0.0822956 ]\n",
      " [ 0.24917188  0.4231973  -0.43854418]\n",
      " [ 0.12620583  0.04300426 -0.1991784 ]]\n",
      "[-0.21706288  0.09333088  0.22692391]\n"
     ]
    }
   ],
   "source": [
    "print(W_o)\n",
    "print(U_o)\n",
    "print(b_o)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('deep_learning_code')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94ae5b382a51005fca8da01575be2d2af1d447d51e335e66e884b29c6c326d4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
